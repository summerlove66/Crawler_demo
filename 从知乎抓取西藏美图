
import requests
from bs4 import BeautifulSoup
from  multiprocessing.dummy import Pool
from urllib.request import urlretrieve
import json
import socket

socket.setdefaulttimeout(5.0) 

k = 0

url = 'https://www.zhihu.com/search?type=content&q=%E8%A5%BF%E8%97%8F'

headers = {
    'Host': 'www.zhihu.com',
    'Referer': 'https://www.zhihu.com/search?type=content&q=%E8%A5%BF%E8%97%8F',
    'Upgrade-Insecure-Requests': '1',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.87 Safari/537.'

}


def get_how(num):
    for i in range(0, num, 10):
        res1 = requests.get('https://www.zhihu.com/r/search?q=%E8%A5%BF%E8%97%8F&type=content&offset={}'.format(i),
                            headers=headers)
        jd = json.loads(res1.text)
        html1 = ''.join(jd['htmls'])
        bs = BeautifulSoup(html1, 'lxml')
        for link in bs.select('.title a'):
            yield 'https://www.zhihu.com' + link.get('href')


def getimg(l):
    global k

    res3 = requests.get(l, headers=headers)

    bs = BeautifulSoup(res3.text, 'lxml')
    for ele in bs.select("img.lazy"):
        try:

            urlretrieve(ele.get('data-original'), 'F:\zhihuxz\{}.jpg'.format(k))  
        except:
            continue
        k += 1


if __name__ == '__main__':
    pool = Pool(4)
    pool.map(getimg, get_how(50))
