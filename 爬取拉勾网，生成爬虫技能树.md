###1.爬取拉勾网.北京（爬虫关键字）前9页，并将职位描述部分写到本地
```
import requests
from bs4 import BeautifulSoup
import json
import time
from multiprocessing.dummy import Pool
change_url ='http://www.lagou.com/jobs/positionAjax.json?city=%E5%8C%97%E4%BA%AC&needAddtionalResult=false'



f =open('pachong.txt','w',encoding='utf8')



links =[]


s =requests.session()
for i in range(1,10):
    time.sleep(2)
    if i ==1:
        datas ={'first':'true','pn':i,'kd':'爬虫' }
    else:
        datas={'first':'false','pn':i,'kd':'爬虫' }
    res3 = s.post(change_url,data=datas)
    jd = json.loads(res3.text)      # 解析 json


    for i in (jd['content']['positionResult']['result']):
        links.append("http://www.lagou.com/jobs/" + str(i['positionId']) + '.html')


def getsrc (url ):
    html =requests.get(url).text
    bs =BeautifulSoup(html,'lxml')
    jobdes =bs.select('dd.job_bt')[0]
    f.write(jobdes.text.strip()[5:])
    f.write('\n')

pool =Pool(4)

pool.map(getsrc,links)
f.close()
```
###2.对文本进行分词处理（jieba 超好用）然后写个简单的过程  完成聚类
```
import jieba


def is_alphabet(uchar):
    """判断一个unicode是否是英文字母"""
    if (uchar >= u'\u0041' and uchar <= u'\u005a') or (uchar >= u'\u0061' and uchar <= u'\u007a'):
        return True
    else:
        return False



f = open('pachong.txt', encoding='utf8')
dic = {}
for ele in jieba.cut(f.read()):
    if is_alphabet(ele):     #提取英文单词
        ele =ele.upper()   #统一首字母大写样式 ，防止重复统计
        if ele not in dic:
            dic[ele] = 1
        else:
            dic[ele] = dic[ele] + 1

for k in dic.keys():
    if dic[k]>=5:  # 统计出现5次以上的

        print(k.capitalize(),dic[k])
```
###3.利用teableau进行可视化
效果：
https://public.tableau.com/profile/jinhui6340#!/vizhome/lagou_beijing_java/Sheet1

